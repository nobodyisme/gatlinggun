#!/usr/bin/env python
from ConfigParser import ConfigParser
import functools
from multiprocessing import Process
import os.path
import sys
import time

from kazoo.client import KazooClient
from mastermind.service import ReconnectableService
from mastermind.utils.queue import LockingQueue
import msgpack
from opster import command
import tornado
from tornado import web

from gatlinggun.config import JsonConfig
from gatlinggun import daemon
from gatlinggun.errors import EllipticsError, InvalidDataError
from gatlinggun.gun import Gun
from gatlinggun import helpers as h
from gatlinggun.logger import logger, setup_logger
from gatlinggun import monitor
from gatlinggun import sync


DEFAULT_LOGGING_CONFIG_PATH = '/etc/elliptics/gatlinggun/logging-default.conf'
LOGGING_CONFIG_PATH = '/etc/elliptics/gatlinggun/logging.conf'

DEFAULT_PIDFILE = '/var/run/elliptics-gatlinggun/gatlinggun'


@command(usage='-c CONFIG -l LOGGING_CONFIG'
               '[--daemonize] [--pidfile PIDFILE] [--user USER]')
def main(config=('c', '', 'Application config file'),
         log_config=('l', '', 'Application logging config file'),
         daemonize=('', False, 'Daemonize this process'),
         pidfile=('', None, 'Use this pidfile'),
         user=('', None, '')):
    """This script starts gatling gun daemon that
    distributes keys to designated cache nodes"""

    main_config = ConfigParser({'read_chunk_size': None,
                                'write_chunK_size': None})

    if not config:
        raise RuntimeError('Application config is required (-c|--config)')

    if not main_config.read(config):
        raise RuntimeError('Failed to open config file: {0}'.format(config))

    use_log_config = DEFAULT_LOGGING_CONFIG_PATH
    if log_config:
        use_log_config = log_config
    elif os.path.exists(LOGGING_CONFIG_PATH):
        use_log_config = LOGGING_CONFIG_PATH

    try:
        if daemonize:
            user = user or main_config.get('global', 'user')
            d = daemon.Daemon(pidfile or DEFAULT_PIDFILE, user)
            d.run = run
            d.start(main_config, use_log_config)
        else:
            run(main_config, use_log_config)
    except Exception as e:
        logger.exception('Main process failed: {0}'.format(e))
        sys.exit(1)


def run(config, log_config):
    logging_config = JsonConfig(log_config)
    setup_logger(logging_config['logging'])

    if config.has_option('global', 'monitor_port'):
        p = Process(target=monitor_app, args=(config,))
        p.start()

    node = h.make_elliptics_node(config)

    gun = Gun(
        node,
        config.get('elliptics', 'cache_path_prefix'),
        config.get('global', 'tmp_dir'),
        read_chunk_size=config.get('elliptics', 'read_chunk_size'),
        write_chunk_size=config.get('elliptics', 'write_chunk_size'),
        request_id_prefix=config.get('elliptics', 'request_id_prefix'),
    )
    zk_addresses = config.get('zookeeper', 'addresses')
    logger.info('Connection to transport hosts %s' % zk_addresses)
    if isinstance(zk_addresses, unicode):
        zk_addresses = zk_addresses.encode('utf-8')
    client = KazooClient(zk_addresses)
    client.start()
    task_queues = dict(
        (group_id, LockingQueue(client, config.get('zookeeper', 'task_path'), group_id))
        for group_id in gun.local_cache_groups
    )
    # transport = Transport(gun.local_cache_groups,
    #     config.get('zookeeper', 'task_path'),
    #     host=zk_addresses, timeout=3)

    def update_local_cache_groups(task_queues):
        gun.update_local_cache_groups()
        group_ids = set()

        for group_id in gun.local_cache_groups:
            group_ids.add(group_id)
            task_queues.setdefault(
                group_id,
                LockingQueue(client, config.get('zookeeper', 'task_path'), group_id)
            )

        for lost_group_id in set(task_queues.keys()) - group_ids:
            del task_queues[lost_group_id]

    sync.PeriodThread(
        functools.partial(update_local_cache_groups, task_queues),
        config.getint('global', 'sync_keys_period')
        if config.has_option('global', 'sync_keys_period') else
        10
    ).start()

    mastermind = ReconnectableService(
        config.get('mastermind', 'app'),
        addresses=config.get('mastermind', 'addresses'),
    )
    # synchronizer = sync.Synchronizer(node, group, transport, service)
    # pth = sync.PeriodThread(synchronizer.sync_keys,
    #                         config.getint('global', 'sync_keys_period')).start()

    logger.info('Starting task processing')

    max_attempts = int(config.get('global', 'max_task_attempts'))
    task_attempts = {}

    def consume_task(task):
        try:
            task.consume()
        except Exception:
            logger.exception(
                'Failed to consume item, path {path}'.format(
                    path=task.path
                )
            )
            pass

    def notify_mastermind(task, cache_group, success):
        if task['action'] == Gun.DISTRUBUTE_TASK_ACTION and success:
            task_status = 'upload_success'
        elif task['action'] == Gun.DISTRUBUTE_TASK_ACTION and not success:
            task_status = 'upload_failed'
        elif task['action'] == Gun.REMOVE_TASK_ACTION and success:
            task_status = 'removal_success'
        elif task['action'] == Gun.REMOVE_TASK_ACTION and not success:
            task_status = 'removal_failed'
        else:
            raise ValueError('Bad value for mastermind notification: {task}'.format(task=task))

        logger.info(
            'Key {key}: notifying mastermind with status "{status}"'.format(
                key=task['key'],
                status=task_status,
            )
        )
        mastermind.enqueue(
            'update_cache_key_status',
            msgpack.packb({
                'id': task['key'],
                'couple': task['couple'],
                'cache_group': cache_group,
                'status': task_status,
            })
        ).get()

    while True:
        logger.info('Fetching task from queue')

        time.sleep(1)

        for cache_group, task_queue in task_queues.items():
            try:
                for item in task_queue:

                    try:
                        task = msgpack.unpackb(item.data)
                        if 'key' not in task:
                            raise ValueError('Task does not have key set')
                        if 'couple' not in task:
                            raise ValueError('Task does not have couple set')
                    except Exception:
                        logger.exception(
                            'Invalid data item, path {path}'.format(
                                path=item.path,
                            )
                        )
                        consume_task(item)

                        continue

                    key = task['key']

                    if key in task_attempts:
                        # check next attempt time
                        next_attempt = task_attempts[key]['next_attempt']
                        seconds_left = next_attempt - time.time()
                        if seconds_left > 0:
                            logger.info(
                                'Key {key} will be skipped, next attempt '
                                'in {seconds} seconds'.format(
                                    key=key,
                                    seconds=seconds_left,
                                )
                            )
                            continue

                    try:

                        logger.info(
                            'Key {key}: task: {task}'.format(
                                key=task['key'],
                                task=task,
                            )
                        )

                        gun.process(task)

                        logger.info(
                            'Key {key}: processing successfully finished'.format(
                                key=task['key'],
                            )
                        )

                    except EllipticsError as e:
                        if e.elliptics_error_code in Gun.RETRIABLE_ELLIPTICS_ERRORS:
                            task_record = task_attempts.setdefault(
                                key,
                                {
                                    'attempts': 0,
                                    'next_attempt': None,
                                }
                            )
                            task_record['attempts'] += 1
                            if task_record['attempts'] >= max_attempts:
                                logger.info(
                                    'Key {key}: max attempts limit reached, '
                                    'removing from tasks'.format(
                                        key=key,
                                    )
                                )
                                task_attempts.pop(key, None)
                                consume_task(item)
                                notify_mastermind(task, cache_group, success=False)
                                continue
                            else:
                                task_record['next_attempt'] = next_attempt_time(task_record, config)
                                logger.info(
                                    'Key {key}: will be retried in {seconds} seconds'.format(
                                        key=key,
                                        seconds=task_record['next_attempt'] - int(time.time()),
                                    )
                                )
                                continue

                        logger.error(
                            'Key {key}: elliptics error is not retriable, '
                            'task will be removed from queue'.format(
                                key=key,
                            )
                        )
                        # critical elliptics error on key distribution,
                        # task cannot be processed,
                        # task attemps history can be cleaned up
                        task_attempts.pop(key, None)
                        consume_task(item)
                        notify_mastermind(task, cache_group, success=False)

                    except InvalidDataError as e:
                        logger.error(
                            'Key {key}: task has invalid format ({error}) '
                            'task will be removed from queue'.format(
                                key=key,
                                error=e,
                            )
                        )

                        # key task has invalid structure, task cannot be processed,
                        # task attemps history can be cleaned up
                        task_attempts.pop(key, None)
                        consume_task(item)
                        notify_mastermind(task, cache_group, success=False)
                    else:
                        # distributed key successfully, task attemps history can
                        # be cleaned up
                        task_attempts.pop(key, None)
                        consume_task(item)
                        notify_mastermind(task, cache_group, success=True)

            except Exception as e:
                logger.exception('Unexpected error')

            finally:
                time.sleep(1)


def next_attempt_time(task_record, config):
    attempts = task_record['attempts']

    delay = int(config.get('global', 'task_delay'))
    exp = int(config.get('global', 'task_delay_exp'))
    max_delay = int(config.get('global', 'task_max_delay'))

    next_delay = min(delay * (exp ** (attempts - 1)), max_delay)

    return int(time.time()) + next_delay


def monitor_app(config):
    mon = monitor.Monitor(config)
    application = web.Application([
        (r"/stats/", monitor.MonitorHandler, {'monitor': mon})
    ], gzip=True)
    application.listen(config.get('global', 'monitor_port'))
    tornado.ioloop.IOLoop.instance().start()


if __name__ == '__main__':
    main.command()
